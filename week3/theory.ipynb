{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Week 3 Theoretical Exercises\n", "It is very imporant that you **try** to solve every exercise. It is not important that you answer correctly. Spend no more than 5-10 min on each exercise. If you do not solve the exercise, focus on understanding the question, and try to figure out what it is you do not understand.\n", "\n", "The TA's will be very happy to answer questions during the TA session or on the board. \n", "\n", "The Last exercises may be quite difficult so do not use all your time there if you are unable to solve them."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise: Linear Regression Presentation\n", "Give a 5-7 min talk about linear regression as you would present it at an exam.\n", "* Always start with defining the problem, what do i want and when i have it how do i use it..\n", "* Define the optimization goal \n", "* explain strategy to attack  the optimization problem. \n", "* Derive the learning algorithm. - State types and shapes of all variables in play.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise: Linear Regression and the missing inverse\n", "In linear regression, given data matrix $X$ and labels vector $y$ the optimal weight vector $w$ (minimizing $\\|Xw-y\\|_2^2$, is found simply by computing the matrix product\n", "$$\n", "(X^\\intercal X)^{-1}X^\\intercal y\n", "$$\n", "That only makes sense if $(X^\\intercal X)$ is in fact invertible.\n", "\n", "In class i suggested that if indeed $(X^\\intercal X)$ is not invertible then we should remove linear dependent columns from $X$.\n", "In this exercise you must argue that this is a good idea.\n", "\n", "To do this, you must prove/argue the two following two things\n", "* Removing linear dependent columns from X does not change the cost of an optimal solution $w$\n", "* If $(X^\\intercal X)$ is not invertible then $X$ contains linear dependent columns\n", "\n", "HINT 1: you can use a well known linear algebra fact that rank(X) = rank($X^\\intercal X$) = rank($X X^\\intercal$)\n", "\n", "HINT 2: $Xw$ is in the column space of $X$"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "# in python code\n", "p1 = 2.0\n", "p2 = 3.0\n", "p = np.array([p1, p2])\n", "phi_p = np.array([1.0, p1, p2, p1**2, p2**2, p1*p2 ])\n", "print('p -> phi(p): ', p, ' -> ', phi_p)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise: Convex Functions \n", "\n", "See https://en.wikipedia.org/wiki/Convex_function for the definitions of convex functions.\n", "There are three ways to prove a functions is convex. Use the easiest one for each task :).\n", "\n", "Which of the following functions are convex on ${\\mathbb R}$? \n", "\n", "-   $f(x) = 2$\n", "\n", "-   $f(x) = -\\ln (x), x>0$\n", "\n", "-   $f(x) = x^3$\n", "\n", "-   $f(x) = x^2 + x^4$\n"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "# Lets plot them\n", "x = np.linspace(-1, 1, 1000)\n", "xp = x[x>0]\n", "plt.figure(figsize=(10,8))\n", "plt.plot(x, [2 for y in x], 'r-', label='f(x)=2')\n", "plt.plot(xp, [-np.log(z) for z in xp], 'g-', label='f(x)=ln(x), x>0')\n", "plt.plot(x, x**3, 'b-', label='f(x)=x^3')\n", "plt.plot(x, x**2 + x**4, 'y-', label='f(x)=x^2+x^4')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise: Maximum Likelihood Linear Regression\n", "\n", "The Linear Regression method may also be derived as a maximum likelihood\n", "procedure. In linear regression the function we learn is choosen to\n", "minimize mean squared error, a criterion that we introduced more or less\n", "arbitrarily.\n", "\n", "I.e. Given X, y compute\n", "$$\n", "w_{\\textrm{opt}} = \\textrm{argmin}_w: \\sum_{i=1}^n (w^\\intercal x_i - y_i)^2\n", "$$\n", "We now revisit Linear Regression from the point of view of maximum\n", "likelihood estimation. \n", "\n", "We consider the target function a conditional distribution $p(y\n", "| x)$ and assume it is defined as\n", "$$\n", "p(y\\mid x,w) = w^\\intercal x + \\varepsilon, \n", "$$ \n", "for some unknown $w$, where $\\varepsilon$ is a\n", "noise term independent of $x$ that is normally distributed with zero\n", "mean and variance $\\sigma^2$ i.e. \n", "$$\n", "\\mathbb{E}[\\varepsilon] = 0, \\mathbb{E}[\\varepsilon^2] =\\sigma^2\n", "$$\n", "\n", "For the (1D) normal distribution with mean $\\mu$ and variance $\\sigma^2$\n", "the probability density function is\n", "$$\n", "p(x) = \\frac{1}{\\sqrt{2\\sigma^2\\pi}}e^{-(x-\\mu)^2/2\\sigma^2}\n", "$$\n", "\n", "In other words, given $x$, the target function  outputs a value $y$ that is\n", "distributed as a Gaussian (normal distribution) around $w^\\intercal\n", "x$. \n", "We can now write $p(y\\mid x,w)$ as\n", "$$\n", "p(y \\mid x,w ) = \\frac{1}{\\sqrt{2\\sigma^2\\pi}}e^{-(y-w^\\intercal x)^2/2\\sigma^2}\n", "$$\n", "for some unknown $w$ that we wish to learn. \n", "*We have plotted a data set from such a function in the next cell below (after the out of sample question).*\n", "\n", "We want to make an algorithm that computes the maximum likelihood\n", "parameters $w_\\textrm{ml}$ of our model. We are given a data set $D\n", "= \\{(x_i, y_i) \\mid i = 1, \\dots, n\\}$ and for a fixed $w$ we let $P(D\n", "\\mid w) = \\prod_{i=1}^n p(y_i \\mid x_i ,w)$ be the likelihood of the\n", "data given $w$. Your job is to derive an algorithm for computing the\n", "maximum likelihood parameters, namely\n", "$w_\\mathrm{ml} = \\operatorname*{arg\\,max}_w\n", "P(D \\mid w)$.\n", "\n", "**Hint:** Minimize the negative log likelihood of the data instead and note\n", "that we end up with a formula for computing $w$ that should look\n", "familiar.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercise:  Show that the cost function for Logistic Regression is convex (Hard Exercise)\n", "### Try at least the first part\n", "\n", "In class we derived that for Logistic Regression the Negative Log Likelihood (NLL)\n", "that we needed to find the minizing parameters for is defined as \n", "$$\n", "NLL(w) = - \\sum_{i=1}^n y_i \\lg (\\sigma (w^\\intercal x)) + (1-y_i) \\lg (1 -\\sigma (w^\\intercal x))\n", "$$\n", "We need to prove that NLL(w) is a convex function (data X, y fixed as usual).\n", "A sum of convex functions is convex so we can ignore the sum and focus on just one element.\n", "i.e. we need to show that\n", "$$\n", "f(w) = - y_i \\lg (\\sigma (w^\\intercal x)) - (1-y_i) \\lg (1 -\\sigma (w^\\intercal x))\n", "$$\n", "is a convex function.\n", "\n", "In the following we let $p=\\sigma(w^\\intercal x)$ so ease up the writing\n", "\n", "We will do this in simple steps. First let us assume that x and w are 1D vectors i.e. numbers.\n", "To prove that $f$ is convex we can prove that $f''(w) >= 0$ for all $w$.\n", "* Step 1. Prove that $f'(w) = - y \\cdot x(1-\\sigma(w x)) + (1-y) \\cdot x \\cdot \\sigma(w x) = - x(y - \\sigma(w x))$ # check signs\n", "* Step 2. Prove that $f''(w) =  x \\cdot x  (\\sigma(w x) \\cdot (1 - \\sigma (w x)) $\n", "* Step 3. Argue that $f''(w) >= 0$ for all w\n", "\n", "As for Linear Regression we can compute the gradient in a forward and a backwards pass completely automated. See below.\n", "\n", "To generalize this to d-dimensional $w$ and $x$ the same steps apply except now we have to do vector derivatives. Oh Oh.\n", "We will do it in python below. \n", "\n", "* Step 1. Show that the Jacobian of f is  $(y - \\sigma(w^\\intercal x)) x^\\intercal$, note that the gradient is the tranpose of that\n", "\n", "The Hessian matrix can be considered related to the Jacobian matrix by $H(f(x)) = J(\\nabla f(x))^\\intercal$.\n", "I.e. it is the Jacobian of the Gradient, or the Jacobian of the transposed Jacobian if you will.\n", "So define the Gradient as $g(w) = x^\\intercal (y-\\sigma(w^\\intercal x)$ and compute derivatives of w again to get the jacobian. Note that the gradient is a function from $\\mathbb{R}^d \\rightarrow \\mathbb{R}^d $ so the jacobian of that is a $d \\times d$ matrix.\n", "\n", "* Step 2. Show that the Hessian of is $p (1-p) x x^\\intercal$ (note that this is an outer product)\n", "\n", "* Step 3. Show that $x x^\\intercal $ is a Positive Semidefinite Matrix, i.e. $v^\\intercal x x^\\intercal v >= 0$ for all vectors v, Argue that the Hessian is a PSD.\n", "\n"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "# arbitrary data and w\n", "from scipy.special import expit as sigmoid\n", "\n", "w = np.array([1, 2,]).reshape(2,1)\n", "x = np.array([1, 2]).reshape(2, 1)\n", "y = np.array([1]).reshape(1,1)\n", "# forward pass\n", "print('Lets compute the NLL nice and slow')\n", "z = w.T @ x\n", "p = sigmoid (z)\n", "l1 = np.log(p)\n", "l2 = np.log(1-p)\n", "f1 = y * l1\n", "f2 = (1-y) * l2\n", "nll = -(f1+f2)\n", "\n", "print('NLL of w on data x,y: ', nll)\n", "print('Let us Rule the Chain backwards again until we find w - somewhat tricy')\n", "dnll_f1 = -1\n", "dnll_f2 = -1\n", "df1_l1 = y\n", "df2_l2 = (1-y)\n", "dl1_p = 1/p\n", "dl2_p = -1/(1-p)\n", "\n", "print('Here we have two derivatives using p. The almighty CHAIN RULER says ADD THEM UP to get partial derivative of nll after p. ')\n", "print('So please for every time we see p compute partial derivate of nll as a function of p, d_nll/d_p and ADD THEM UP')\n", "dnnl_l1 = dnll_f1 * df1_l1 # chain rule d_nnl/d_l1 = d_nll/d_f1 * d_f1/d_l1\n", "dnnl_l2 = dnll_f2 * df2_l2\n", "dnnl_p = dnnl_l1 * dl1_p + dnnl_l2 * dl2_p\n", "print('Now we have partial derivative of nll as a function of p and we know that d_nll/d_w = d_nll/d_p * d_p/d_w')\n", "dp_z = p*(1-p)\n", "dz_w = x.T\n", "J_nll = dnnl_p * dp_z * dz_w\n", "print('The jacobian shape:', J_nll.shape)\n", "print('The jacobian it self:', J_nll)\n", "print('Reading off what we wrote the Jacobian it is: (-1 *y * 1/p - (1-y) * (-1/(1-p))) * p * (1-p) * x.T which is:\\n -x(y - \\sigma(w.T @ x))')\n", "print('Since p = sigmoid(w.T x)')\n", "print('Another win for the machine - Now repeat to find the Hessian...')"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["# Cross Entropy - What is it (For the very very interested student)\n", "This is not an exercise but some extra material for the interested student. Why is our cost called Cross Entropy.\n", "In sense this is the cost function we always almost use.\n", "Since this is not a course in information theory we will just provide a link and a video for the interested student.\n", "\n", "See the Khan academy video https://www.khanacademy.org/computing/computer-science/informationtheory/moderninfotheory/v/information-entropy\n", "for an explanation of entropy.\n", "\n", "If you understand that read the following to see if you can understand why it is called Cross Entropy.\n", "https://www.quora.com/Whats-an-intuitive-way-to-think-of-cross-entropy\n", "\n"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["## Excercise: Nonlinear Transforms (Hard Exercise for the Combinatorial People)\n", "\n", "As seen in class we can transform the data to make nonlinear fits of the input data even when we are still learning linear models.\n", "The simplest is the polynomial transform that was also shown at the lecture.\n", "Assume the input is $n$ data points $x_1,\\dots, x_n$of $d$ dimensions.\n", "For the second degree polynomial transform we make this into all polynomials of max total degree at most two.\n", "I.e. if d = 2 and the input point is $p = (p_1, p_2)$ this becomes \n", "$$\n", "\\phi(p) = (1, p_1, p_2, p_1^2, p_2^2, p_1 p_2)\n", "$$\n", "which is 6 dimensional. **Notice that the order is irrelevant**\n", "\n", "* Still using input dimension d=2, how many dimensions in the transformed points if we instead use degree 3 polynomial transform? What about a $k$ degree transform?\n", "* For general input dimension $d$ and polynomial transform target dimension $k$ how many features do you get?\n", "* How long will it take to compute the transform?\n", "\n", "\n", "\n", "**Hint:** http://mathworld.wolfram.com/BallPicking.html\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.0"}}, "nbformat": 4, "nbformat_minor": 2}