{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Week 9 Exercises\n", "Last Exercises from the supervised learning part. Get Busy. But remember someone should talk about the hand in.\n", "There are only two exercises besides hand in talk but they are both quite heavy on the implementation side."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ex 1: Hand In 2 Walk Through\n", "Preferably by the students!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ex 2: One Question for the Professor - Implementing Regression Stumps?\n", "In this exercise your task is to implement  Regression Trees with one internal node also known as Regression Stumps.\n", "Regression stumps are used for Regression with Least Squares loss.\n", "\n", "This means that the learning algorithm has to find the best possible feature to split the training data in regards with Least Squares loss.\n", "\n", "We have decided to present a Regression Stump by \n", "- idx: the data/feature vector index to consider in the root node (the one question asked)\n", "- val: the value to compare to for data feature idx in the root node\n", "- left: the value to return for a data point if it ends up in left leaf (x[idx] < val)\n", "- right: the value to return for a data point if it ends up in the right leaf (x[idx] >= val)\n", "\n", "See **regression_stumps.py** for starter code.\n", "\n", "**You need to complete the RegressionStump class by completing the following methods**\n", "- implement predict \n", "- implement score\n", "- implement fit\n", "\n", "**What is the running time of your algorithm?**\n", "\n", "\n", "**hint:** It is fine to implement a slow version for finding the best split.\n", "For the people who wants a faster version they may notice that $\\sum_{i=1}^n (x_i - \\mu)^2 = \\sum_i x_i^2 + \\sum_i \\mu^2 - 2 \\mu \\sum_i x_i = \\sum_i x_i^2 + n \\cdot \\mu^2 - 2 \\mu \\sum_i x_i $\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ex 3: Gradient Boosting for Regression\n", "Now that we can fit regression trees with least squares loss we can use them to train boosted models for any (smooth) loss. \n", "\n", "Your task is to implement a Gradient Boosting algorthm for regression with Mean Absolute Value cost function  which is $L(h(x), y) = \\lvert h(x)-y\\rvert$.\n", "For data $(x_i, y_i), i=1,\\dots, n$ this gives\n", "\n", "$$\n", "\\frac{1}{n} \\sum_{i=1}^n \\lvert h(x_i)-y_i\\rvert\n", "$$\n", "\n", "We have decided to represent a gradient boosted model by.\n", "- constant_value $f_0$, \n", "- model_list list of trees $T_1,\\dots,T_k$ and corresponding weights $\\alpha_1,\\dots, \\alpha_k$  (all $\\alpha_i$ will be identical to day but they do not have to in general)\n", "\n", "and the prediction of a given model gb is then\n", "\n", "$$\n", "\\textrm{gb}(x) = f_0 + \\sum_{i=1}^k \\alpha_i T_i(x)\n", "$$\n", "\n", "To use that for regression prediction, given data point $x$ you simply return $\\textrm{gb}(x)$.\n", "\n", "The least absolute value loss for the model on a given data point is simply defined as\n", "\n", "$$\n", "L(y, \\textrm{gb}(x)) = \\lvert y - \\textrm{gb}(x) \\rvert\n", "$$\n", "\n", "\n", "\n", "Your task is to complete the GradientBoosterRegression class in gradient_boosting_regression.py.\n", "\n", "But before you start you need to derive a formula for\n", "\n", "$$\n", "\\frac{\\partial L(x, \\textrm{gb}(x))}{\\partial  \\textrm{gb}(x))}\n", "$$\n", "\n", "and then you need to figure out how output the correct value in a leaf.  \n", "This boils down to for a given set of elements indexed by index set $I$ compute the value $w$ minimizing.\n", "$$\n", "\\sum_{i\\in I} L(y_i,\\textrm{gb}(x)+w) = \\sum_{i\\in I} \\lvert y_i - (\\textrm{gb}(x_i)+w) \\rvert\n", "$$\n", "\n", "\n", "\n", "The learning algorithm is in a few words as follows.\n", "\n", "- Start by computing the optimal constant value i.e. $\\textrm{argmin}_w \\sum_{i=1}^n L(y_i, w)$\n", "- Then do m rounds as follows:\n", "- First the algorithm computes the above derivative of every training data point and constructs a regression stump that fits the negative of that (data is (x, -grad L(x, gb(x))))\n", "- After learning the regression tree, the learning algorithm updates the leafs of the tree to return the value that minimize the least absolute value cost \n", "- Finally the learned tree is then scaled by a learning rate (shrinkage [ESL]) and added to the model which becomes a sum of scaled Regression Stumps. You can tune learning rate if you like!\n", "\n", "With this derivative in hand you your task is to complete the following methods in the Gradient Booster class (gradient_boosting.py) (See alg. 10.3 in [ESL] for more info if needed)\n", "- implement evaluate\n", "- implement score (predict made for you...)\n", "- implement cost_grad\n", "- implement fix_leaf_values\n", "- implement fit\n", "\n", "Now download [gradient_boosting_regression.py](gradient_boosting.py) and start implementing. \n", "\n", "You can run the algorithm with different number of trees (the main method) too see how the model develops over time.\n", "\n", "What is the running time of your algorithm?\n"]}, {"cell_type": "markdown", "metadata": {}, "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.0"}}, "nbformat": 4, "nbformat_minor": 2}