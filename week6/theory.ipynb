{"cells": [{"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["# Week 6 Exercises"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ex 1: Hand In 1 Walk Through\n", "- Explain logistic regression and explain the cost and gradient computation? You can show your code!\n", "- How hard is to to add weight decay regularzation to Logistic Regression and Softmax? How do you update cost and gradient?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ex 2:  Maximal Margin By Hand \n", "- Assume you are given two points $x_1=(1, 1)$ with class -1 and $x_2 = (11, 11)$ with class 1. What is the maximum separating hyperplane. E.g. what are vectors w and b that maximally separates these points? What are the support vectors? What is the margin? Can you find the parameters w, b the SVM wil find without having to do the SVM path and instead just thinking a little. \n", "- If we have three points in class -1 (-10, -10), (-5, 2), (1, 1) and four points in class 1 (20, 23), (15, 17), (12, 10), (10, 12). What is the separating hyperplane (w,b)? What are the support vectors? What is the margin? You can run the code below to get the answers from actually running the python sklearn SVM implementation on the data.\n", "- Write down the exact form of the SVM problem we need to solve if the input data is defined as the 7 points above.\n", "The convex quadratic program was defined as \n", "\n", "$\\min_{w, b} \\frac{1}{2}{||w||^2}$\n", "\n", "s.t. $y_i(w^\\intercal x_i + b) \\geq 1$\n", "\n", "You job is thus to write down the constraints.\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ex 3: Regularized SVM\n", "\n", "Consider a regularized SVM wrong side of the margin penalty C.\n", "\n", "$\\min_{w, b, \\xi} \\frac{1}{2}{||w||^2} + C \\sum_{i=1}^n \\xi_i$\n", "\n", "s.t. $y_i(w^\\intercal x_i + b) \\geq 1 - \\xi_i$\n", "\n", "and $\\xi_i \\geq 0$\n", "\n", "Assume you are given three points $x_1=(1, 1)$ with class -1 and $x_2=(3, 3), x_3 = (11, 11)$ with class 1.\n", "- Write down the exact form of the SVM problem we need to solve if the input data $D=\\{x_1, x_2, x_3\\}$.\n", "  Thus, your job is thus to write down the constraints.\n", "\n", "- What is the best cost you can get when using the hyperplane $w = [ 0.1, 0.1]$ and  $b: -1.2$. i.e. how can you pick $\\xi_1, \\xi_2, \\xi_3$ such that the constraints are satisfied while minimizing the cost with w and b  fixed.\n", "\n", "- Is there a general way given, $w$ and $b$ to compute the best $\\xi_i$ \n", "- Can you find a $w, b$ with a smaller cost for $C=1$\n", "  If you like you can use the svm code below to experiment.\n", "  \n", "Also be sure to see how to apply an SVM implementation on data using Sklearn.\n"]}, {"cell_type": "code", "execution_count": 182, "metadata": {"scrolled": false}, "outputs": [], "source": ["# code for running the example above\n", "%matplotlib inline\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from sklearn import svm\n", "\n", "def plot_hyperplane(w, *args, **kwargs): \n", "    if w[1]==0 and w[2]==0:\n", "        raise ValueError('Invalid hyperplane')\n", "    xmin, xmax, ymin, ymax = plt.axis()\n", "    \n", "    if w[2]==0:\n", "        # Vertical line\n", "        x = np.array((1/w[1], 1/w[1]))\n", "        y = np.array((ymin, ymax))\n", "    else:\n", "        x = np.array((xmin, xmax))\n", "        y = (-w[0]-w[1]*x)/w[2]       \n", "    # plot the line\n", "    plt.plot(x, y, *args, **kwargs)\n", "\n", "def run_svm(X, Y, kernel='linear', **kwargs):\n", "    # fit the model\n", "    clf = svm.SVC(kernel=kernel, **kwargs)\n", "    clf.fit(X, Y)\n", "\n", "    # get the separating hyperplane\n", "    \n", "    print('Hyperplane found w: {0} b: {1}'.format(clf.coef_[0],clf.intercept_[0]))\n", "    margin = 1.0/np.linalg.norm(clf.coef_[0])\n", "    print('Margin 1/||w||: {0}'.format(margin))\n", "    hyp = np.r_[clf.intercept_,clf.coef_[0]]\n", "    \n", "    plt.figure(figsize=(10, 8))\n", "    plt.scatter(X[:, 0], X[:, 1], c=Y,cmap=plt.cm.Paired)\n", "    print('Support Vectors:')    \n", "    print(clf.support_vectors_)\n", "    \n", "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],s=80, facecolors='none')\n", "    plt.xlim(X.min()-1,X.max()+1)\n", "    plt.ylim(X.min()-1,X.max()+1)\n", "    plot_hyperplane(hyp,'k-')\n", "    \n", "    print('what is hyp',hyp)\n", "    tmp = hyp[0]\n", "    hyp[0] = tmp + 1\n", "    plot_hyperplane(hyp,'k--')\n", "    hyp[0] = tmp -1\n", "    plot_hyperplane(hyp,'k--')\n", "    hyp[0] = tmp\n", "    \n", "    #plt.axis('tight')\n", "    plt.show()\n", "\n", "X = np.array([[1, 1], [11, 11]])\n", "Y = np.array([0, 1])\n", "print('First Exercise From Above')\n", "run_svm(X, Y, C=1)\n", "plt.show()\n", "\n", "print('Second Exercise From Above')\n", "X = np.array([(-10, -10), (-5, 2), (1, 1),  (20, 23), (15, 17), (12, 10), (10, 12)])\n", "Y = np.array([0, 0, 0, 1, 1, 1, 1])\n", "run_svm(X, Y)\n", "plt.show()\n", "\n", "# print('Larger Separable Data set')\n", "# n = 50\n", "# X = 2*np.r_[2*np.random.randn(n, 2) - [4, 4], 2*np.random.randn(n, 2) + [5, 5]]\n", "# Y = [-1] * n + [1] * n\n", "# run_svm(X, Y)\n", "# plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ex 4: Kernels\n", "In class, we saw that it was often possible to compute $K(x, z) = \\langle \\phi(x), \\phi(z) \\rangle$ without using or knowing $\\phi$, and implicitly map\n", "data to a high dimensional space, and have the SVM algorithm work in that space. One\n", "way to generate kernels is to explicitly define the mapping $\\phi$ to a higher dimensional space,\n", "and then work out the corresponding $K$ if possible.\n", "\n", "In this question we are interested in direct construction of kernels. I.e., suppose\n", "we have a function $K(x, z)$ that we think gives an appropriate Kernel for our\n", "learning problem, and we are considering plugging K into the SVM as the kernel function.\n", "However for $K(x, z)$ to be a valid kernel, it must correspond to an inner product in some\n", "higher dimensional space resulting from some feature mapping $\\phi$. Mercer\u2019s theorem tells\n", "us that $K(x, z)$ is a (Mercer) kernel if and only if for any finite set $\\{x_1,\\dots, x_m\\}$, the\n", "matrix $K$ is symmetric and positive semidefinite, where the square matrix \n", "$K \\in R^{m \\times m}$ is\n", "given by $K_{ij} = K(x_i, x_j)$\n", "\n", "Now here comes the questions: \n", "Let $K_1, K_2$ be kernels over $R^n \\times R^n$, let $a \\in R_+$ be a positive\n", "real number, let $f : R^n \\rightarrow R$ be a real-valued function, \n", "let $\\phi : R^n \\rightarrow R^d$ be a function mapping from $R^n$ to $R^d$\n", "let $K_3$ be a kernel over $R^d \\times R^d$\n", "and let p(x) a polynomial over x with positive coefficients.\n", "\n", "For each of the functions K below, state whether it is necessarily a kernel. If you think it\n", "is, prove it (give feature transform or prove PSD); if you think it isn\u2019t, give a counter-example.\n", "* $K(x, z) = K_1(x, z) + K_2(x, z)$ \n", "* $K(x, z) = aK_1(x, z)$\n", "* $K(x, z) = K_1(x, z) \u2212 K_2(x, z)$\n", "* $K(x, z) = \u2212aK_1(x, z)$\n", "* $K(x, z) = f(x)f(z)$\n", "\n", "### Harder\n", "* $K(x, z) = K_3(\\phi(x), \\phi(z))$\n", "* $K(x, z) = K_1(x, z)K_2(x, z)$ (Hard Exercise: may be hard to prove - if so ignore)\n", "* $K(x, z) = p(K_1(x, z))$\n", "\n", "\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ex 5: Kernel Speed\n", "In this exercise we will test if if using Kernels is indeed faster than doing the transform explicitly.\n", "\n", "Your task is to fit a polynomial kernel svm with degree 2 on the data and try to do a feature transform of degree 2 and run a linear svm and compare the time.\n", "\n", "Use timeit to compare time, only consider the fit time.\n", "\n", "Use sklearns polynomial transform to do the [polynomial transform](http://scikit-learn.org/stable/modules/preprocessing.html#generating-polynomial-features)\n"]}, {"cell_type": "code", "execution_count": 185, "metadata": {}, "outputs": [], "source": ["import timeit\n", "import os \n", "import numpy as np\n", "from sklearn.preprocessing import PolynomialFeatures\n", "from sklearn import svm\n", "\n", "def load_test_data():\n", "    \"\"\" Load and return the test data \"\"\"\n", "    filename = 'auTest.npz';\n", "    if not os.path.exists('auTest.npz'):\n", "        os.system('wget https://users-cs.au.dk/jallan/ml/data/auTest.npz')        \n", "    tmp = np.load('auTest.npz')\n", "    au_digits = tmp['digits']\n", "    print('shape of input data', au_digits.shape)\n", "    au_labels = np.squeeze(tmp['labels'])\n", "    print('labels shape and type', au_labels.shape, au_labels.dtype)\n", "    return au_digits, au_labels\n", "\n", "D, y = load_test_data()\n", "n = 400\n", "Ds = D[0:n, :]\n", "ys = y[0:n]\n", "# use only Ds and ys\n", "\n", "def poly_fit(X, y, kernel, degree):\n", "    clf = svm.SVC(kernel=kernel, degree=degree)\n", "    clf.fit(X, y)\n", "    return clf\n", "\n", "# You can use poly fit with kernel='poly' and degree =2\n", "# kernel='linear' and degree = 1 i.e. timeit poly_fit ...\n", "### YOUR CODE HERE\n", "### END CODE\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ex 6: Hinge Loss - Cost and Gradient\n", "In class we talked about that we could rewrite the primal problem (with no kernels) into an unconstrained minimization problem using hinge loss.\n", "For a hypothesis and data point $x$ the hinge loss is defined as\n", "$$\n", "e(h(x), y=f(x)) = \\max(0, 1-h(x)y)\n", "$$\n", "Define the (average regularized) hinge loss  over the data set as\n", "$$\n", "\\min_{w, b}: \\frac{1}{n} \\sum_{i=1}^n \\max(0, 1-h(x_i)y_i) + \\lambda ||w||^2 = \\frac{1}{n} \\sum_{i=1}^n \\max(0,1 -(w^\\intercal x_i +b) y_i) + \\lambda ||w||^2\n", "$$\n", "What is the relation between the SVM penalty $C$ and $\\lambda$\n", "\n", "To make a gradient descent algorithm for Hinge Loss for a linear model which you must\n", "compute the cost and the gradient. Write the code for computing the Hinge Loss and the Gradient at a given value $w$ and $\\lambda$ in the hinge_loss method in the cell below. You should derive the math formula for the gradient before you implement it!.\n", "\n", "\n", "\n", "\n", "\n", "(we know max is not smooth at zero but let us ignore that and say that the gradient is zero at zero, for more  go to wikipedia and look up subgradient).\n"]}, {"cell_type": "code", "execution_count": 158, "metadata": {}, "outputs": [], "source": ["n = 50\n", "X = 2*np.r_[2*np.random.randn(n, 2) - [4., 4.], 2*np.random.randn(n, 2) + [5., 5.]]\n", "Y = np.array([-1] * n + [1] * n)\n", "\n", "def hinge_loss(X, y, w, b, reg):\n", "    \"\"\" Compute hinge loss on data with given parameters\n", "    \n", "    Args:\n", "        X: np.array shape n,d \n", "        y: np.array shape n, \n", "        w: np.array shape d,\n", "        b: is float\n", "        reg: is float        \n", "    Returns:\n", "    output scalar, grad_w np.array shape d, grad_b scalar\n", "    \"\"\"\n", "    loss = 0\n", "    reg_loss = 0\n", "    grad_w = np.zeros(w.shape)\n", "    grad_b = 0\n", "    \n", "### YOUR CODE HERE - compute loss, reg_loss, grad_w, grad_b\n", "### END CODE\n", "\n", "    assert grad_w.shape == w.shape    \n", "    return loss + reg_loss, grad_w, grad_b\n", "\n", "w = np.array([-1, -1])\n", "b = 1.0\n", "reg = 1.0\n", "reg_loss, grad_w, grad_b = hinge_loss(X, Y, w, b, reg)\n", "print('hinge loss: ', reg_loss, 'hinge grad_w: ', grad_w, 'hinge grad_b:', grad_b)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Ex 7: Hinge Loss Gradient Descent In Pytorch\n", "In this exercise we will see how to use pytorch build in optimizer class to do gradient descent for Hinge Loss.\n", "You can read about the torch optim class that implements most SGD algorithm variants [here](https://pytorch.org/docs/stable/optim.html)\n", "1. Implement Hinge Loss in Pytorch in method hinge_loss - only the cost from above using torch functionality and not numpy\n", "2. Test your your implementation from this and the previous exercise by checking that the cost and gradient are the same for both implmentations. In particular check that the gradient from above matches the gradient produced by pytorch.\n", "3. Test that the pytorch gradient descent finds the minimum (fast) and notice how to apply the optimizer."]}, {"cell_type": "code", "execution_count": 163, "metadata": {"scrolled": false}, "outputs": [], "source": ["import torch\n", "import torch.optim as optim\n", "import numpy as np\n", "\n", "\n", "hinge_data = torch.from_numpy(X).float()\n", "hinge_labels = torch.from_numpy(Y).float()\n", "w = torch.tensor([-1., -1], requires_grad=True)\n", "b = torch.tensor([1.0], requires_grad=True)\n", "reg = 1.0\n", "\n", "def hinge_loss(data, labels, w, b, lbd):\n", "    \"\"\"\n", "    Compute Hinge Loss on Torch tensors\n", "    torch.sum and x.clamp may com in usefull\n", "    https://pytorch.org/docs/master/torch.html#torch.clamp\n", "    \"\"\"\n", "    ### YOUR CODE HERE\n", "    ### END CODE\n", "    return out\n", "\n", "print('Check your computations')\n", "loss = hinge_loss(hinge_data, hinge_labels, w, b, reg)\n", "loss.backward()\n", "print('check gradient loss: ', loss.item(),'w grad: ', w.grad, 'b grad:', b.grad)\n", "\n", "steps = 5\n", "optimizer = optim.SGD([w, b], lr = 0.1)\n", "for i in range(steps):\n", "    optimizer.zero_grad() # clears gradients otherwise they are accumulated\n", "    loss = hinge_loss(hinge_data, hinge_labels, w, b, 0)\n", "    loss.backward()\n", "    acc = torch.sum(((hinge_data @ w + b) * hinge_labels)>0)\n", "    print('round: {0}, loss: {1}, accuracy: {2}'.format(i, loss.item(), acc.item()))\n", "    optimizer.step() # take a gradient step\n", "print('Learned w and b', w, b)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ex 8: String Kernel with Sklearn \n", "In this exercise you must apply SVM for text classification by using a Bag Of Words String Kernel on the industri code data we have seen several times.\n", "\n", "The bag of words Kernel $K$ we will use is defined as follows\n", "Given two strings x, z (lists of words separated by space) the Kernel K is defined as the number of distinct words that appear in both x and z\n", "Thus K('a b c a', 'a a b d') = 2 since $a$ and $b$ are the only words that appear in both inputs.\n", "\n", "Expain which feature transform this corresponds to?\n", "\n", "Unfortunately Sklearns implementation of SVMs except the input to be numerical values.\n", "We circumvent this issues by making an index that maps an integer key to the string and encode our data sets as an array of indices.\n", "\n", "The Kernel receives this  encoding of the data set and must from the indices extract the required strings and apply the Kernel to them.\n", "\n", "Your task is to implement the bow_kernel below!\n", "\n", "Note that SVM expect a vectorized Kernel implementation that takes as input two numpy arrays $X, Z$ with shape $(n_1, d)$ and $(n_2, d)$  and outputs the Kernel matrix $K$ of size $n_1 \\times n_2$ where $K_{i,j} = K(X[i], Z[j])$, thus your implementation must be vectorized as well.\n", "\n", "1. Implement the bow_kernel\n", "2. Manually compute the bow kernel matrix on the simple test, data_dict[-1], data_dict[-2], data_dict[-3]\n", "3. Test your implementation gets the same result (add \n", "4. Run the SVM and see if it works (comment in the code - may take some time) - we get an in sample accuray around 98 percent and out of sample around 95 percent."]}, {"cell_type": "code", "execution_count": 186, "metadata": {"scrolled": false}, "outputs": [], "source": ["from sklearn import svm\n", "import os, urllib\n", "from sklearn.model_selection import train_test_split\n", "import pandas as pd\n", "\n", "def load_branche_data(keys):\n", "    \"\"\"\n", "    Load the data in branche_data.npz and save it in lists of\n", "    strings and labels (whose entries are in {0,1,..,num_classes-1})\n", "    \"\"\"\n", "    filename = 'branchekoder_formal.gzip'\n", "    if not os.path.exists(filename):\n", "        with open(filename, 'wb') as fh:\n", "            path = \"http://users-cs.au.dk/jallan/ml/data/{0}\".format(filename)\n", "            fh.write(urllib.request.urlopen(path).read())\n", "    data = pd.read_csv(filename, compression='gzip')\n", "    actual_class_names = []\n", "    features = []\n", "    labels = []\n", "    for i, kv in enumerate(keys):\n", "        key = kv[0]\n", "        name = kv[1]\n", "        strings = data[data.branchekode == key].formal.values\n", "        features.extend(list(strings))\n", "        label = [i] * len(strings)\n", "        labels.extend(label)\n", "        actual_class_names.append(name)\n", "    assert len(features) == len(labels)\n", "    features = np.array(features)\n", "    labels = np.array(labels)\n", "    return features, labels, actual_class_names\n", "\n", "\n", "keys = [(561010, 'Restauranter'), (620100, 'Computerprogrammering')]\n", "features, labels, actual_class_names = load_branche_data(keys)\n", "# Hack to get around sklearn svm issues. Fit only supports numpy arrays so each string is represented by index in data_dict\n", "data_dict = {i: string for (i, string) in enumerate(features)}\n", "feature_indices = np.array(list(data_dict.keys())).reshape(-1, 1)\n", "X_train, X_test, y_train, y_test = train_test_split(feature_indices, labels)\n", "\n", "def bow_kernel(d1, d2):\n", "    \"\"\" bag of words kernel\n", "    \n", "    Args:\n", "    d1: np.array shape (n1, 1)\n", "    d2: np.array shape (n2, 1)\n", "    \n", "    \n", "    the split command may be very useful 'i am a string'.split(' ')\n", "    and the set data structure may be useful as well\n", "    \n", "    returns np.array shape (n1, n2)\n", "    \"\"\"\n", "    n1 = d1.shape[0]\n", "    n2 = d2.shape[0]\n", "    out = np.zeros((n1, n2))\n", "    # d1_1 = data_dict[d1[0][0]]\n", "    # print('frist string id d1: ', d1_1)\n", "    ### YOUR CODE HERE data_dict[d1[0][0]] gives the first string\n", "    ### END CODE\n", "    return out\n", "data_dict[-1] = 'test string 42'\n", "data_dict[-2] = 'another test string 32'\n", "data_dict[-3] = 'can you dig it'\n", "\n", "print('Lets test kernel before we run the learning algorithm. You should be able to check the answer manually')\n", "bow_kernel_test = bow_kernel(np.array([-1, -2, -3]).reshape(-1, 1), np.array([-1, -2, -3, -2]).reshape(-1, 1))\n", "print(bow_kernel_test)\n", "\n", "# print('*'*10, 'Lets fit string kernel svm', '*'*10)\n", "# bow_svm = svm.SVC(kernel=bow_kernel)\n", "# bow_svm.fit(X_train, y_train)\n", "# print('In Sample Score: ', bow_svm.score(X_train, y_train))\n", "# print('Out of Sample Score: ', bow_svm.score(X_test, y_test))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ex 9: Kernel Perceptron (Hard Exercise)\n", "In this exercise you must implement Kernel Perceptron for learning from a stream of data.\n", "The learning algorithm will only do one pass over the data like in a streaming system.\n", "\n", "The important part is how to actually add Kernels to the Perceptron learning algorithm.\n", "\n", "First we need to represent the hyperplane in the feature space induced by the Kernel.\n", "This must be implemented in the class *Representer* as described below.\n", "\n", "Remebering the Perceptron algorithm, the current solution hyperplane is updated on a mispredicted data point $(x, y)$ as\n", "$$\n", "w = w + y x\n", "$$\n", "In this exercise the hyperplane exists in feature space and must be updated there but in the same way\n", "with one difference. We include a learning rate $\\alpha>0$ that scales the update.\n", "The update that must be implemente  becomes\n", "$$\n", "w = w + \\alpha y \\phi(x)\n", "$$\n", "where $\\phi$ is the feature transform corresponding to the used Kernel. As we will see we do not really need to know what $\\phi$ is but it does need to exist.\n", "\n", "This means that the hyperplane solution is a linear combination of (transformed) inputs points and thus can be written as\n", "$$\n", "w = \\sum_i \\alpha_i \\phi(x_i)\n", "$$\n", "and may be represented by storing the list of $\\alpha_i$ and $x_i$. Note we have not discussed how to initialize $w$, that can be done just like an update. Also note that we do not use a bias variable in this exercise.\n", "\n", "\n", "**Task:** In the class Representer implement \n", "* update(x, $\\alpha$): (add point x with weight $\\alpha$ to the hyperplane\n", "* dot(z): compute and return \n", "$$\n", "\\langle w, \\phi(z) \\rangle = \\langle \\sum_i  \\alpha_i \\phi(x_i), \\phi(z) \\rangle = \\sum_i \\alpha_i K(x_i, z)\n", "$$ \n", "(note the indexing here is not over the data set but the set of weight and points comprising w - and we assume the lists are non-empty)\n", "\n", "After you have implemented the representation of the hyperplane you must implement\n", "The Pereptron Classifier in the *KernelPerceptron* class.\n", "* Implement the score function (compute accuracy of classifier on given data X with labels y)\n", "* Implement the fit method - do one scan over the data and for each misprediction (x, y) update $w$ by adding $\\alpha y \\phi(x)$\n", "\n", "Test your implementation by running the cell."]}, {"cell_type": "code", "execution_count": 166, "metadata": {}, "outputs": [], "source": ["# streaming perceptron with kernels\n", "import os, urllib\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "def load_data():\n", "    \"\"\" Simple helper function for downloading and loading data \"\"\"\n", "    filename = 'nonlinear_data.npz'\n", "    if not os.path.exists(filename):\n", "        os.system('wget https://users-cs.au.dk/jallan/ml/data/{0}'.format(filename))\n", "    D = np.load(filename)\n", "    return D\n", "\n", "def visualize_kernel_perceptron(X, Y, w):\n", "    \"\"\" Helper function for visualizing decision boundary in input space\"\"\"\n", "    fig, ax = plt.subplots(figsize=(20, 16))\n", "    ax.scatter(X[:, 0], X[:,1 ], c=Y, cmap=plt.cm.Paired, s=20)\n", "    nsize = 50\n", "    xs = ys = np.linspace(-1, 1, nsize)\n", "    xm, ym = np.meshgrid(xs, ys)\n", "    img = np.zeros((nsize, nsize)) # makes a 100 x 100 2d array\n", "    for i, zy in enumerate(ys):\n", "        for j, zx in enumerate(xs):    \n", "            point = np.array([zx, zy])\n", "            predict = w.dot(point)\n", "            img[i, j] = predict\n", "    ax.contour(xs, ys, img, [0], colors='r', linewidths=3)\n", "    plt.show()\n", "\n", "def get_rbf_kernel(gamma=1.0):\n", "    assert gamma > 0, 'Gamma must be positive'\n", "    def K(x, z):\n", "        return np.exp((-gamma * np.sum((x-z)**2)))\n", "    return K\n", "\n", "class Representer():\n", "    \"\"\" Represents a hyperplane H in Feature space that is a linear combination of transformed points from input space\n", "        \n", "        The class can evaluate an input in the original input space mapped to H against the hyperplane        \n", "    \"\"\"\n", "    def __init__(self, K):\n", "        self.dat = list()\n", "        self.alpha = list()\n", "        self.K = K\n", "\n", "    def update(self, x, a):\n", "        \"\"\" Update hyperplane Representer by point x with weight a i.e. w = w +  * x\n", "        \n", "        Args:\n", "            x: np.array - data point\n", "            a: float - data weight\n", "        \"\"\"\n", "        ### YOUR CODE HERE\n", "        ### END CODE\n", "\n", "    def dot(self, z):\n", "        \"\"\" compute inner product between hyperplane and z in feature (kernel) space\n", "        i.e. <phi(w), phi(z)>\n", "        \n", "        Args:\n", "            z: np.array\n", "        \"\"\"\n", "        ### YOUR CODE HERE\n", "        ### END CODE\n", "        return out\n", "    \n", "class KernelPerceptron():\n", "    \n", "    def __init__(self, K):\n", "        self.K = K\n", "        self.w = None\n", "    \n", "    def fit(self, X, Y, K, alpha=0.1):\n", "        \"\"\" Kernel Perception Algorithm \n", "            1. Set initial hyperplane i.e. make representation of w to initial value like phi(0) with weight 1\n", "            2. Do one pass over the data - for each misprediction add phi(x) with weight y * alpha to hyperplane representation\n", "            \n", "        \"\"\"\n", "        w = Representer(self.K)\n", "        ### YOUR CODE HERE\n", "        ### END CODE\n", "        \n", "    def score(self, X, y):\n", "        \"\"\" Compute Classifier Accuracy\n", "        Args:\n", "          X: np.array n, d\n", "          y: np.array n, \n", "        \n", "        Returns:\n", "        out: scalar - accuracy of model on data X with labels Y\n", "        \"\"\"\n", "        out = None\n", "        ### YOUR CODE HERE\n", "        ### END CODE\n", "        return out\n", "            \n", "    \n", "\n", "D = load_data()\n", "X = D['X4']\n", "Y = D['y4']\n", "rbf_kernel = get_rbf_kernel(1.0)\n", "kernel_perceptron_classifier = KernelPerceptron(rbf_kernel)\n", "kernel_perceptron_classifier.fit(X, Y, rbf_kernel)\n", "print('In Sample Accuracy after one scan: {0}'.format(kernel_perceptron_classifier.score(X, Y)))\n", "visualize_kernel_perceptron(X, Y, kernel_perceptron_classifier.w)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.0"}}, "nbformat": 4, "nbformat_minor": 2}