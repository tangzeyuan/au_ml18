{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%qtconsole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization (theory)\n",
    "Please see slides 17-30 in `ML-Clustering-L2.pdf` for an explanation of the Expecation Maximization algorithm. Let $D = \\{x_1,...,x_n\\}\\subseteq \\mathbb{R}^d$. Recall that in the EM algorithm we represent a clustering as a probability distribution over the points. The probability that a point, $x$, belongs to some cluster $C_i$ is assumed to be Gaussian, i.e.:\n",
    "\n",
    "$$\n",
    "\\Pr(x\\ | \\ C_i) = \\frac{1}{\\sqrt{(2\\pi)^d|\\Sigma_i|}}\\cdot e^{-\\frac{1}{2}\\cdot (x-\\mu_i)^\\intercal\\cdot (\\Sigma_i)^{-1}\\cdot (x-\\mu_i)}, \\ \\quad i=1, \\dots k \\ \\qquad (1).\n",
    "$$\n",
    "\n",
    "Where $|\\Sigma_i |$ denotes the determinant of $\\Sigma_i$.\n",
    "\n",
    "\n",
    "\n",
    "Each cluster $C_i$ is parameterized with a cluster mean $\\mu_i\\in\\Bbb R^d$, a covariance matrix $\\Sigma_i\\in\\Bbb R^{d\\times d}$ and a prior probability $\\Pr(C_i)$.\n",
    "\n",
    "<!--Given a clustering $M = \\{C_1,\\ldots,C_k\\}$, we can compute\n",
    "$$\n",
    "\\Pr(x) = \\sum_{i=1}^k \\Pr(C_i)\\cdot\\Pr(x|C_i),\n",
    "$$\n",
    "where we estimate $\\Pr(C_i)$ by $W_i$, the frequency of the cluster $C_i$ in the data (i.e. the ratio of data points belonging to cluster $C_i$).-->\n",
    "\n",
    "The Expectation Maximization has two steps: Expectation and Maximization (hence the name). Before these steps we need to initialize a clustering $M = \\{C_1,\\ldots,C_k\\}$. A (very) simple heuristic for initialization is:\n",
    "- Choose cluster mean $\\mu_i\\in\\Bbb R^d$ uniformly at random (in the adequate region). \n",
    "- Initialize the covariance matrix $\\Sigma_i\\in\\Bbb R^{d\\times d}$ as the identity matrix. \n",
    "- Initialize the prior probability $\\Pr(C_i)$ to $\\frac{1}{k}$. \n",
    "\n",
    "\n",
    "Then we repeat the following two steps until the sum $\\sum_{i=1}^k \\|\\mu_i - \\mu'_i\\|$ is below some pre-specified threshold $\\epsilon$ (where $\\mu'_i$ and $\\mu_i$ are the means computed in two consecutive executions).\n",
    "\n",
    "- <b>Update probabilities (expectation step):</b> For all pairs $C_i$ and $x_j$ compute $\\Pr(C_i|x_j) = \\frac{\\Pr(x_j \\ | \\ C_i)\\cdot \\Pr(C_i)}{\\Pr(x_j)}$. To do this we need to compute $\\Pr(x_j \\ | \\ C_i)$ and $\\Pr(x_j)$. Compute $\\Pr(x_j \\ | \\ C_i)$ by the formula (1) above and compute $$\\Pr(x_j)= \\sum_{i=1}^k \\Pr(C_i)\\cdot\\Pr(x_j \\ | \\ C_i).$$ \n",
    "  \n",
    "- <b>Update clustering (maximization step): </b>Compute a new model by recomputing $W_i \\approx \\Pr(C_i)$, $\\mu_i$ and $\\Sigma_i$ as\n",
    "\n",
    "$$\n",
    "W_i = \\frac{1}{n}\\cdot\\sum_{x\\in D}\\Pr(C_i \\ | \\ x)\\approx P(C_i),\n",
    "\\qquad\n",
    "\\mu_i = \\frac{\\sum_{x\\in D}^n x\\Pr(C_i \\ | \\ x)}{\\sum_{x\\in D} \\Pr(C_i \\ | \\ x)},\n",
    "\\qquad\n",
    "\\Sigma_i = \\frac{\\sum_{x\\in D} \\Pr(C_i \\ | \\ x)\\cdot(x-\\mu_i)\\cdot(x-\\mu_i)^\\intercal}{\\sum_{x\\in D} \\Pr(C_i \\ | \\ x)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 1:</b> What is the objective function of Expectation Maximization (EM) clustering problem? \n",
    "\n",
    "HINT: See slides pages 19. \n",
    "\n",
    "<!--<b>Question 1 (b): </b>Rewrite the objective for $\\text{dist}(x,y)=||x-y||$-->\n",
    "\n",
    "<b>Question 2:</b> What are the E-step, M-step? What is estimated, what is fixed?\n",
    "\n",
    "<b>Question 3:</b> Does EM algorithm provide any guarantees for finding the optimal solution?\n",
    "\n",
    "HINT: What is the optimal solution? \n",
    "\n",
    "<b>Question 4 (b): </b>Could we assign a \"hard clustering\" of all points instead of a probability?\n",
    "\n",
    "\n",
    "<b>Question 5:</b> Can a cluster in a hard EM clustering be empty? \n",
    "\n",
    "<b>Question 6:</b> Is the EM algorithm guaranteed to converge? \n",
    "\n",
    "HINT: Assume the objective is strictly decreasing between every step of the algorithm. \n",
    "\n",
    "<b>Question 7 (a): </b>How is Expectation Maximization Clustering a generalizaton of k-means clustering?\n",
    "\n",
    "HINT: See the book [ZM] page 353. \n",
    "\n",
    "<b>Question 7 (b): </b>Both models rely on certain assumptions on the data. What are these assumptions? Which assumptions are shared?\n",
    "\n",
    "<b>Question 7 (c): </b>Give examples of data where the assumptions of k-means clustering and EM clustering are reasonable/unreasonable. \n",
    "\n",
    "HINT: Draw data that satisfies/doesn't satisfy all assumptions for k-means clustering. What about EM clustering? \n",
    "\n",
    "<!--\n",
    "TODO: Some question that I thought about which would be nice to have is like a discussion about the differences between Kmeans and EM. The book points out (page 353) that EM is some kind of generalization of Kmeans, so in some sense it's more versatil (for example, it doesn't restrict us to convex clusters -right?- and it seems like the fact that the clusters are ''randomized'' allows us to do more things). What are the advantages/disadvantages of each? -->\n",
    "\n",
    "\n",
    "# EM-clustering as a Hidden Markov Model (optional)\n",
    "\n",
    "One can also think of the EM-clustering model as a particularly simple HMM. If we consider a model with $k$ clusters, we can define a corresponding HMM with $k$ hidden states. The transition probability from any state $j$ to any state $i$ is identical to the prior probability, $\\Pr(C_i)$. The emission probability in hidden state $i$ is $\\Pr(x \\ | \\ C_i)$, (a gaussian probability distribution). \n",
    "\n",
    "\n",
    "The data set we have, $D =\\{x_1, \\dots x_m \\} $ corresponds to the emissions from the HMM, and the (hard) clustering is the decoding! Since the transition probabilities $\\Pr(z_n = i \\ | \\ z_{n-1} = j) = P(C_i)$ do not depend on $j$, finding a decoding is particularly simple. In fact, the Viterbi and the Posterior decoding will give the same results (why?). The result will be, that $z_j = \\text{argmax}_i \\Pr(x_j \\ | \\ C_i)$. \n",
    "\n",
    "\n",
    "Now, the Baum-Welch training we saw for HMMs summarized in slide 20 in the slide-set hmm-training is *exactly* the algorithm presented above for EM-clustering! You may try to derive this if you have the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization (code)\n",
    "In this exercise you must implement the EM algorithm. To test our implementation we will need data. Like last week we use the Iris dataset. Recall that the dataset has three clases so we <i>cheat</i> by setting $k=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load the Iris data set\n",
    "import sklearn.datasets\n",
    "iris = sklearn.datasets.load_iris()\n",
    "X = iris['data'][:,0:2] # reduce dimensions so we can plot what happens, you may experiment with this.\n",
    "k = 3\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing EM algorithm\n",
    "Remember the Expectation Maximization algorithm has two steps. Let us first implement the Expectation step. For this step it suffices to compute $\\Pr(C_i\\mid x_j)$ for $i=1,\\ldots,k$ and $j=1,\\ldots n$. Remember that $\\Pr(C_i \\mid x_j)$ can be computed as \n",
    "\n",
    "$$\\Pr(C_i|x_j) = \\frac{\\Pr(x_j|C_i)\\cdot \\Pr(C_i)}{\\Pr(x_j)}$$\n",
    "\n",
    "The first goal will then be to compute each part of the above equation. To do this we will need the parameters: ($\\mu_i, \\Sigma_i, \\Pr(C_i)$). These will be represented in Python as follows: \n",
    "\n",
    "$$\\text{means}=\\begin{pmatrix}\n",
    "- & \\mu_1^T & - \\\\\n",
    "- & \\vdots & - \\\\\n",
    "- & \\mu_k^T & - \n",
    "\\end{pmatrix}\\in \\mathbb{R}^{k\\times d} \\quad\\quad\n",
    "\\text{probs_c}=\\begin{pmatrix}\n",
    "\\Pr(C_1)\\\\\n",
    "\\vdots \\\\\n",
    "\\Pr(C_k)\n",
    "\\end{pmatrix}\\in \\mathbb{R}^{k}\n",
    "$$\n",
    "\n",
    "Similarly we represent the $\\Sigma_i$'s as $\\text{covs}\\in\\mathbb{R}^{k\\times d \\times d}$ such that $\\text{covs[i]}=\\Sigma_i$. Finally we represent $\\Pr(x)$'s and $\\Pr(C_i \\mid x)$ as \n",
    "\n",
    "$$\\text{prob_x}=\\begin{pmatrix}\n",
    "\\Pr(x_1) \\\\\n",
    "\\vdots \\\\\n",
    "\\Pr(x_n) \n",
    "\\end{pmatrix}\\in \\mathbb{R}^{n} \\quad\\quad\n",
    "\\text{probs_cx}=\\begin{pmatrix}\n",
    "\\Pr(C_1\\mid x_1) & \\ldots & \\Pr(C_1 \\mid x_n)\\\\\n",
    "\\vdots \\\\\n",
    "\\Pr(C_k \\mid x_1) & \\ldots & \\Pr(C_k \\mid x_n)\n",
    "\\end{pmatrix}\\in \\mathbb{R}^{k\\times n}\n",
    "$$\n",
    "\n",
    "The function *compute_probs_cs* below takes `means`, `probs_c` and `covs` as input and returns `probs_cx` and `probs_x`. \n",
    "\n",
    "<b>Question: </b>What is the dimensions of `probs_cx` and how can we compute it given `means`, `probs_c` and `covs`?\n",
    "\n",
    "<!-- The algorithm returns a clustering $M = \\{C_1,\\ldots,C_k\\}$. This corresponds to a \n",
    " - sequence of means $(\\mu_1,\\ldots,\\mu_k)$ where each $\\mu_i\\in\\Bbb R^d$,\n",
    " - sequence of covariance matrices $(\\Sigma_1,\\ldots,\\Sigma_k)$ where each $\\Sigma_i\\in\\Bbb R^{d\\times d}$,\n",
    " - prior probabilities $(\\Pr(C_1),\\ldots,\\Pr(C_k))$. \n",
    "\n",
    "Given $x$ we can then compute the probability of $x$ conditioned on the $i$'th cluster $Pr(C\\mid x_i)$. If we want to assign $x$ to a specific cluster we compute $(\\Pr(x|C_1),\\ldots,\\Pr(x|C_k))$ and assign $x$ to the `arg max`.-->\n",
    "\n",
    "<!--The following helper function takes a description of a Gaussian Mixture ($\\mu_i$'s, $\\Sigma_i$'s and $\\Pr(C_i)$'s)) and returns the probability densities of each point. We represent the inputs as \n",
    "\n",
    "If you want more information you can take a look at scipy's [multivariate_normal](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.multivariate_normal.html) documentation.-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def compute_probs_cx(points, means, covs, probs_c):\n",
    "    '''\n",
    "    Input\n",
    "      - points:    (n times d) array containing the dataset\n",
    "      - means:     (k times d) array containing the k means\n",
    "      - covs:      (k times d times d) array such that cov[j,:,:] is the covariance matrix of the j-th Gaussian.\n",
    "      - probs_c:   (k) array containing priors\n",
    "    Output\n",
    "      - probs_cx:  (k times n) array such that the entry (i,j) represents Pr(C_i|x_j)\n",
    "    '''\n",
    "    # Convert to numpy arrays.\n",
    "    points, means, covs, probs_c = np.asarray(points), np.asarray(means), np.asarray(covs), np.asarray(probs_c)\n",
    "    \n",
    "    # Get sizes\n",
    "    n, d = points.shape\n",
    "    k = means.shape[0]\n",
    "    \n",
    "    # Compute probabilities\n",
    "    # This will be a (k, n) matrix where the (i,j)'th entry is Pr(C_i)*Pr(x_j|C_i).\n",
    "    probs_cx = np.zeros((k, n))\n",
    "    for i in range(k):\n",
    "        \n",
    "         # Handle numerical issues, these lines are unimportant for understanding the algorithm. \n",
    "        if np.allclose(np.linalg.det(covs[i]), 0):  # det(A)=0 <=> singular. \n",
    "            print(\"Numerical issues, run again. \") \n",
    "            return None, None\n",
    "        \n",
    "        probs_cx[i] = probs_c[i] * multivariate_normal.pdf(mean=means[i], cov=covs[i], x=points)\n",
    "    \n",
    "    # The sum of the j'th column of this matrix is P(x_j); why?\n",
    "    probs_x = np.sum(probs_cx, axis=0, keepdims=True) \n",
    "    assert probs_x.shape == (1, n)\n",
    "    \n",
    "    # Divide the j'th column by P(x_j). The the (i,j)'th then \n",
    "    # becomes Pr(C_i)*Pr(x_j)|C_i)/Pr(x_j) = Pr(C_i|x_j)\n",
    "    probs_cx = probs_cx / probs_x\n",
    "    \n",
    "    return probs_cx, probs_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the basic structure for the EM algorithm. For the Expectation step it calls the above code. You need to finish the Maximization step \n",
    "\n",
    "<b>Question: </b>Where in the code (and how) do we initialize `means`, `covs` and `probs_c`?\n",
    "\n",
    "The above code computed the Expectation step. In the code below you should implement the Maximization step.\n",
    "\n",
    "<b>Question: </b>Which three things do you need to compute in the Maximization step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations\tLLH\n",
      "1 \t\t -18.381793170547887\n",
      "2 \t\t -1.8030235593626902\n",
      "3 \t\t -1.7994581953907764\n",
      "4 \t\t -1.7980275055931885\n",
      "5 \t\t -1.7959514209006322\n",
      "6 \t\t -1.7924234136353134\n",
      "7 \t\t -1.7865397730627237\n",
      "8 \t\t -1.7772320256966123\n",
      "9 \t\t -1.7633128524059465\n",
      "10 \t\t -1.7449817646400767\n",
      "11 \t\t -1.727265538404193\n",
      "12 \t\t -1.715367766394926\n",
      "13 \t\t -1.707946845601865\n",
      "14 \t\t -1.704079568318205\n",
      "15 \t\t -1.7021989891303277\n",
      "16 \t\t -1.700855000752647\n",
      "17 \t\t -1.699554048831472\n",
      "18 \t\t -1.6979496364034135\n",
      "[[inf inf]\n",
      " [inf inf]]\n",
      "nan\n",
      "19 \t\t -1.695668308456152\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "20 \t\t -1.6921441293329222\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "21 \t\t -1.6865975773772188\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "22 \t\t -1.6789501464445047\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "23 \t\t -1.6705205684630309\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "24 \t\t -1.6619841738502636\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "25 \t\t -1.6530393657448725\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "26 \t\t -1.6430876422711898\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "27 \t\t -1.6313184874626165\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "28 \t\t -1.6166526678692696\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "29 \t\t -1.5975040930991362\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "30 \t\t -1.5719048201646229\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/au597881/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py:2022: RuntimeWarning: overflow encountered in det\n",
      "  r = _umath_linalg.det(a, signature=signature)\n",
      "/Users/au597881/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:57: RuntimeWarning: overflow encountered in true_divide\n",
      "/Users/au597881/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py:2022: RuntimeWarning: invalid value encountered in det\n",
      "  r = _umath_linalg.det(a, signature=signature)\n",
      "/Users/au597881/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:54: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/au597881/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:66: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 \t\t -1.541132129158364\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "32 \t\t -1.5167781443765107\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "33 \t\t -1.5081309357147934\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "34 \t\t -1.5069580332464585\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "35 \t\t -1.5068613269462308\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "36 \t\t -1.506853605138667\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "37 \t\t -1.5068532667043077\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "38 \t\t -1.5068533818724832\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "39 \t\t -1.5068534413720853\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "40 \t\t -1.506853461984685\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "41 \t\t -1.5068534684123642\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "42 \t\t -1.506853470339518\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "43 \t\t -1.5068534709078323\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "44 \t\t -1.5068534710741976\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "45 \t\t -1.5068534711227348\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "46 \t\t -1.5068534711368733\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "47 \t\t -1.506853471140989\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "48 \t\t -1.5068534711421866\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "49 \t\t -1.5068534711425348\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "50 \t\t -1.5068534711426362\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "51 \t\t -1.5068534711426655\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "52 \t\t -1.506853471142674\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "53 \t\t -1.5068534711426764\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "54 \t\t -1.5068534711426773\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "55 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "56 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "57 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "58 \t\t -1.5068534711426773\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "59 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "60 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "61 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "62 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "63 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "64 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "65 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "66 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "67 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "68 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "69 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "70 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "71 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "72 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "73 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "74 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "75 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "76 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "77 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "78 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "79 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "80 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "81 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "82 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "83 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "84 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "85 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "86 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "87 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "88 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "89 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "90 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "91 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "92 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "93 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "94 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "95 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "96 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "97 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "98 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "99 \t\t -1.5068534711426778\n",
      "[[nan nan]\n",
      " [nan nan]]\n",
      "nan\n",
      "100 \t\t -1.5068534711426778\n"
     ]
    }
   ],
   "source": [
    "def em_algorithm(X, k, T, epsilon = 0.001, means=None):\n",
    "    \"\"\" Clusters the data X into k clusters using the Expectation Maximization algorithm. \n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Data matrix of shape (n, d)\n",
    "        k : Number of clusters.\n",
    "        T : Maximum number of iterations\n",
    "        epsilon :  Stopping criteria for the EM algorithm. Stops if the means of\n",
    "                   two consequtive iterations are less than epsilon.\n",
    "        means : (k times d) array containing the k initial means (optional)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        means:     (k, d) array containing the k means\n",
    "        covs:      (k, d, d) array such that cov[j,:,:] is the covariance matrix of \n",
    "                   the Gaussian of the j-th cluster\n",
    "        probs_c:   (k, ) containing the probability Pr[C_i] for i=0,...,k. \n",
    "        llh:       The log-likelihood of the clustering (this is the objective we want to maximize)\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    \n",
    "    # Initialize and validate mean\n",
    "    if means is None: \n",
    "        means = np.random.rand(k, d)\n",
    "\n",
    "    # Initialize cov, prior\n",
    "    probs_x  = np.zeros(n) \n",
    "    probs_cx = np.zeros((k, n)) \n",
    "    probs_c  = np.zeros(k) + np.random.rand(k)\n",
    "    \n",
    "    covs = np.zeros((k, d, d))\n",
    "    for i in range(k): covs[i] = np.identity(d)\n",
    "    probs_c = np.ones(k) / k\n",
    "    \n",
    "    # Column names\n",
    "    print(\"Iterations\\tLLH\")\n",
    "    \n",
    "    close = False\n",
    "    old_means = np.zeros_like(means)\n",
    "    iterations = 0\n",
    "    while not(close) and iterations < T:\n",
    "        old_means[:] = means \n",
    "\n",
    "        # Expectation step\n",
    "        probs_cx, probs_x = compute_probs_cx(X, means, covs, probs_c)\n",
    "        if probs_cx is None: return em_algorithm(X, k, T, epsilon = epsilon)\n",
    "        assert probs_cx.shape == (k, n)\n",
    "        \n",
    "        # Maximization step\n",
    "        # YOUR CODE HERE\n",
    "        for i in range(k): # loop over clusters\n",
    "            probs_c[i] = 1 / n * np.sum(probs_cx[i, :])\n",
    "            means[i] = probs_cx[i, :] @ X / np.sum(probs_cx[i, :])\n",
    "            for j in range(n):\n",
    "                covs[i] += np.outer(X[j] - means[i], X[j] - means[i]) * probs_cx[i, j]\n",
    "            covs[i] /= np.sum(probs_cx[i, :])\n",
    "        # END CODE\n",
    "        \n",
    "        # Compute per-sample average log likelihood (llh) of this iteration     \n",
    "        llh = 1/n*np.sum(np.log(probs_x))\n",
    "        print(iterations+1, \"\\t\\t\", llh)\n",
    "\n",
    "        # Stop condition\n",
    "        dist = np.sqrt(((means - old_means) ** 2).sum(axis=1))\n",
    "        close = np.all(dist < epsilon)\n",
    "        iterations += 1\n",
    "        \n",
    "    # Validate output\n",
    "    assert means.shape == (k, d)\n",
    "    assert covs.shape == (k, d, d)\n",
    "    assert probs_c.shape == (k,)\n",
    "    return means, covs, probs_c, llh\n",
    "\n",
    "means, covs, priors, llh = em_algorithm(X, 3, 100, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random initialization usually causes the algorithm to get stuck at different local maxima. This causes different runs to get different scores. In practice this can for example be handled by running the algorithm several times and picking the best run. \n",
    "\n",
    "The following code runs EM algorithm 50 times and plots the score of each run. Because the data set is fairly small, $n=150$, most of the runs will get almost the same score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,3))    \n",
    "llhs = []\n",
    "\n",
    "for i in range(100):\n",
    "    _, _, _, llh = em_algorithm(X, 3, 100)\n",
    "    llhs.append(llh)\n",
    "    ax.plot(llhs, 'bx')\n",
    "    fig.canvas.draw() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check your implementation you should run <a href=\"http://scikit-learn.org/stable/modules/mixture.html\">sklearn</a>'s implementation of the EM algorithm. You might want to take a look at the documentation to get a better understanding of what the algorithm is actually doing.\n",
    "\n",
    "<!--By default the implementation repeats the algorithm $10$ times and picks the best result. A sanity check for your implementation of Lloyd's algorithm is to check that the scores are roughly the same. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture as EM\n",
    "expectation_maximization = EM(n_components=3, init_params='random', covariance_type='diag', verbose=2, verbose_interval =1).fit(X)\n",
    "\n",
    "print(expectation_maximization.score(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a visual understanding of the algorithm, the following code visualizes each step of the algorithm. Similarly to previous week, it prints the centroids and the corresponding cluster each point is assigned to. Furthermore, because the clusters are represented as gaussians, the gaussians are plotted. They are first plotted by themselves, and then they are plotted together (by summing). \n",
    "\n",
    "To run the visualization you need to copy and paste your implementation from above. Then the code should run with no modifications. Running the visualization might be a bit slow; try change the `detail` parameter if you have any problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def em_algorithm_visualize(X, k, T, epsilon = 0.001, means=None, detail=20):\n",
    "    \"\"\" Clusters the data X into k clusters using the Expectation Maximization algorithm. \n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Data matrix of shape (n, d)\n",
    "        k : Number of clusters.\n",
    "        T : Maximum number of iterations\n",
    "        epsilon :  Stopping criteria for the EM algorithm. Stops if the means of\n",
    "                   two consequtive iterations are less than epsilon.\n",
    "        means : (k times d) array containing the k initial means (optional)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        means:     (k, d) array containing the k means\n",
    "        covs:      (k, d, d) array such that cov[j,:,:] is the covariance matrix of \n",
    "                   the Gaussian of the j-th cluster\n",
    "        probs_c:   (k, ) containing the probability Pr[C_i] for i=0,...,k. \n",
    "        llh:       The log-likelihood of the clustering (this is the objective we want to maximize)\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    \n",
    "    # Visualization stuff. \n",
    "    fig, (ax, _, _, _, _) = plt.subplots(5, 1, figsize=(10,16)) \n",
    "    ax.axis('off')\n",
    "    colors = [\"r\", \"g\", \"b\"]\n",
    "    ax3d = fig.add_subplot(2, 1, 2, projection='3d')\n",
    "    ax3d1 = fig.add_subplot(3, 1, 2, projection='3d')\n",
    "    ax3d2 = fig.add_subplot(4, 1, 2, projection='3d')\n",
    "    ax3d3 = fig.add_subplot(5, 1, 2, projection='3d')\n",
    "    ax3d.set_axis_off()\n",
    "    ax3d1.set_axis_off()\n",
    "    ax3d2.set_axis_off()\n",
    "    ax3d3.set_axis_off()\n",
    "    \n",
    "    # Initialize and validate mean\n",
    "    if means is None: means = np.random.rand(k, d)\n",
    "\n",
    "    # Initialize \n",
    "    probs_x  = np.zeros(n) \n",
    "    probs_cx = np.zeros((k, n)) \n",
    "    probs_c  = np.zeros(k) \n",
    "    covs = np.zeros((k, d, d))\n",
    "    for i in range(k): covs[i] = np.identity(d)\n",
    "    probs_c = np.ones(k) / k\n",
    "\n",
    "    # END CODE\n",
    "    \n",
    "    # Column names\n",
    "    print(\"Iterations\\tLLH\")\n",
    "    \n",
    "    close = False\n",
    "    old_means = np.zeros_like(means)\n",
    "    iterations = 0\n",
    "    while not(close) and iterations < T:\n",
    "        old_means[:] = means # This allows us to actually copy the array mean\n",
    "\n",
    "        # Expectation step\n",
    "        probs_cx, probs_x = compute_probs_cx(X, means, covs, probs_c)\n",
    "        if probs_cx is None: return em_algorithm(X, k, T, epsilon = epsilon)\n",
    "        assert probs_cx.shape == (k, n)\n",
    "        \n",
    "        # Maximization step\n",
    "        # YOUR CODE HERE\n",
    "        for i in range(k): # loop over clusters\n",
    "            probs_c[i] = 1 / n * np.sum(probs_cx[i, :])\n",
    "            means[i] = probs_cx[i, :] @ X / np.sum(probs_cx[i, :])\n",
    "            for j in range(n):\n",
    "                covs[i] += np.outer(X[j] - means[i], X[j] - means[i]) * probs_cx[i, j]\n",
    "            covs[i] /= np.sum(probs_cx[i])\n",
    "        # END CODE\n",
    "        \n",
    "        # Finish condition\n",
    "        dist = np.sqrt(((means - old_means) ** 2).sum(axis=1))\n",
    "        close = np.all(dist < epsilon)\n",
    "        iterations += 1\n",
    "        \n",
    "        \n",
    "        # !----------- VISUALIZATION CODE -----------!\n",
    "        centroids = means\n",
    "        # probs_cx's (i,j) is Pr[C_i, x_j]\n",
    "        # assign each x_i to the cluster C_i that maximizes P(C_i | x_j)\n",
    "        clustering = np.argmax(probs_cx, axis=0)\n",
    "        assert clustering.shape == (n,), clustering.shape\n",
    "        \n",
    "        # Draw clusters\n",
    "        ax.cla()\n",
    "        for j in range(k):\n",
    "            centroid = centroids[j]\n",
    "            c = colors[j]\n",
    "            ax.scatter(centroid[0], centroid[1], s=123, c=c, marker='^')\n",
    "            data = X[clustering==j]\n",
    "            x = data[:,0]\n",
    "            y = data[:,1]\n",
    "            ax.scatter(x, y, s=3, c=c)\n",
    "            \n",
    "        # draw 3d gaussians. \n",
    "        #Create grid and multivariate normal\n",
    "        xs = np.linspace(4,7, 50)\n",
    "        ys = np.linspace(2,4.5, 50)\n",
    "        Xs, Ys = np.meshgrid(xs, ys)\n",
    "        pos = np.empty(Xs.shape + (2,))\n",
    "        pos[:, :, 0] = Xs; pos[:, :, 1] = Ys\n",
    "        prob_space = sum([multivariate_normal(means[j], covs[j]).pdf(pos) for j in range(k)])\n",
    "\n",
    "        #Make a 3D plot\n",
    "        ax3d.cla()\n",
    "        ax3d1.cla()\n",
    "        ax3d2.cla()\n",
    "        ax3d3.cla()\n",
    "        ax3d.plot_surface(Xs, Ys, prob_space, cmap='viridis', linewidth=0)\n",
    "        ax3d1.plot_surface(Xs, Ys, multivariate_normal(means[0], covs[0]).pdf(pos), cmap='viridis', linewidth=0)\n",
    "        ax3d2.plot_surface(Xs, Ys, multivariate_normal(means[1], covs[1]).pdf(pos), cmap='viridis', linewidth=0)\n",
    "        ax3d3.plot_surface(Xs, Ys, multivariate_normal(means[2], covs[2]).pdf(pos), cmap='viridis', linewidth=0)\n",
    "        \n",
    "        fig.canvas.draw()\n",
    "        \n",
    "    # Validate output\n",
    "    assert means.shape == (k, d)\n",
    "    assert covs.shape == (k, d, d)\n",
    "    assert probs_c.shape == (k,)\n",
    "    return means, covs, probs_c, llhs\n",
    "\n",
    "# Load the Iris data set\n",
    "import sklearn.datasets\n",
    "iris = sklearn.datasets.load_iris()\n",
    "X = iris['data'][:,0:2] # reduce dimensions so we can plot what happens.\n",
    "k = 3\n",
    "\n",
    "# the higher the detail the slower plotting\n",
    "detail = 20 # 50 looks very nice but your computer might not be able to handle it. \n",
    "means, covs, priors, llh = em_algorithm_visualize(X, 3, 40, 0.001, detail=detail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing EM with Lloyd's algorithm \n",
    "\n",
    "So far we have been initializing the means for the Expectation Maximization algorithm at random. We could also make a mix of Lloyd's algorithm and EM algorithm by running Lloyd's algorithm first to obtain the initial means for the EM algorithm. \n",
    "\n",
    "Begin by copying and pasting the implementation of Lloyd's algorithm from the previous week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lloyds_algorithm(X, k, T):\n",
    "    \"\"\" Clusters the data of X into k clusters using T iterations of Lloyd's algorithm. \n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Data matrix of shape (n, d)\n",
    "        k : Number of clusters.\n",
    "        T : Maximum number of iterations to run Lloyd's algorithm. \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        clustering: A vector of shape (n, ) where the i'th entry holds the cluster of X[i].\n",
    "        centroids:  The centroids/average points of each cluster. \n",
    "        cost:       The cost of the clustering \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use this algorithm to initialize the means for the EM algorithm. For this notice that `em_algorithm` accepts an optional input for the initial means.\n",
    "\n",
    "Also, notice that the sklearn's implementation of the EM algorithm can take this initialization into account. Can you look through the documentation and find out what lines should be changed when we used sklearn before? This would be very useful for testing and comparing your implementation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question:</b> Why can we use Lloyd's algorithm to initialize the EM algorithm? Does it (always) give a better final clustering? If so, why is that the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering digits\n",
    "Last week, we performed clustering of digits by the use of Lloyds algorithm. Now, we try the same with the EM clustering algorithm. The following code imports the data set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "\n",
    "X = mnist_trainset.train_data.numpy().reshape((60000, 28*28))\n",
    "y = mnist_trainset.train_labels.numpy()\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code runs the Expectation Maimization algorithm on 5000 images from the MNIST dataset. It then visualizes the found centroids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture as EM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# One cluster for each digit\n",
    "k = 10\n",
    "\n",
    "# Run EM algorithm on 1000 images from the MNIST dataset. \n",
    "\n",
    "expectation_maximization = EM(n_components=k, max_iter=10, init_params='kmeans', covariance_type='diag', verbose=1, verbose_interval =1).fit(X)\n",
    "print(expectation_maximization.score(X[:1000]))\n",
    "\n",
    "means = expectation_maximization.means_\n",
    "covs = expectation_maximization.covariances_\n",
    "      \n",
    "print(means.shape)\n",
    "fig, ax = plt.subplots(1, k, figsize=(8, 1))\n",
    "\n",
    "for i in range(k):\n",
    "    ax[i].imshow(means[i].reshape(28, 28), cmap='gray')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 1: </b>Why do the centroids look like images of digits? \n",
    "    \n",
    "A particularly cool thing about representing a clustering as probability distributions is that it is possible to sample points from them! This way you can generate *new* images of digits. A similar idea lies behind some of the new cool techniques for generation of images such as variational autoencoders. Lets try sampling!\n",
    "\n",
    "We have implemented a function to do this for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "import numpy as np\n",
    "\n",
    "def sample(means, covs, num):\n",
    "    mean = means[num]\n",
    "    cov = covs[num]\n",
    "     \n",
    "    fig, ax = plt.subplots(1, 10, figsize=(8, 1))\n",
    "    \n",
    "    for i in range(10):\n",
    "        img = multivariate_normal.rvs(mean=mean, cov=cov) # draw random sample   \n",
    "        ax[i].imshow(img.reshape(28, 28), cmap='gray') # draw the random sample\n",
    "    plt.show()\n",
    "\n",
    "sample(means, covs, 5)  ##Try changing the number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
