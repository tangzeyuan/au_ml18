{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Week 4 (Theoretical) Exercises\n", "\n", "Do as many as your can. Exercise 10 and 11 are the least important.\n", "\n", "Exercise 9 may be difficult but also important."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Ex 1: Gradient Descent \n", "Let $f_a(x_1, x_2) = \\frac{1}{2}(x_1^2 + a\\cdot x_2^2)$\n", "\n", "Where $a$ is parameter that we will change.\n", "\n", "In the cell below, write a gradient descent algorithm that tries to minimize $f$ starting from point (256, 1), (256 is largest $a$ we try).\n", "Run it for 42 steps and save the sequence of points considered in the algorithm.\n", "\n", "We have added some code to visualize the path taken by gradient descent.\n", "\n", "Try a=1, 4, 16, 64, 128, 256 and adjust the step size to see if you can make it converge \n", "**hint - consider approximately 1/a as stepsize for large a at least**\n", "\n", "What do you see? \n", "\n", "It should be possible for you to figure out what the local (global) minimum is, as well as the gradient.\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "def f(a, x):\n", "    return 0.5 * (x[0]**2 + a * x[1]**2)\n", "\n", "def visualize(a, path, ax=None):\n", "    \"\"\"\n", "    Make contour plot of f_a and plot the path on top of it\n", "    \"\"\"\n", "    x = np.arange(-257, 257, 0.1)\n", "    y = np.arange(-100, 100, 0.1)\n", "    xx, yy = np.meshgrid(x, y)\n", "    z = 0.5 * (xx**2 + a * yy**2)\n", "    if ax is None:\n", "        fig, ax = plt.subplots(figsize=(16, 13))\n", "    h = ax.contourf(xx, yy, z, cmap=plt.get_cmap('jet'))\n", "    ax.plot([x[0] for x in path], [x[1] for x in path], 'w.--', markersize=4)\n", "    ax.plot([0], [0], 'rs', markersize=8) # optimal solution\n", "    ax.set_xlim([-257, 257])\n", "    ax.set_ylim([-100, 100])\n", "\n", "def gd(a, step_size=0.1, steps=42):\n", "    \"\"\" Run Gradient descent\n", "        fill list out with the sequence of points considered during the descent.         \n", "    \"\"\"\n", "    out = []\n", "    ### YOUR CODE HERE    \n", "    ### END CODE\n", "    return out\n", "\n", "fig, axes = plt.subplots(2, 3, figsize=(20, 16))\n", "ateam = [[1, 4, 16], [64, 128, 256]]\n", "for i in range(2):\n", "    for j in range(3):\n", "        ax = axes[i][j]\n", "        a = ateam[i][j]\n", "        path = gd(a) # use good step size here\n", "        visualize(a, path, ax)\n", "        ax.set_title('gd a={0}'.format(a))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Ex 2:  Book Exercises\n", "## Exercise 1.11 and 1.12 in the Book \n", "(Not problems but exercises inside the text. page 25, 26\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Ex 3: Error Functions\n", "\n", "STORY: MI6 story about false negatives and false positives; \n", "\n", "Assume you work for a security company and have a fingerprint scanner.\n", "False Positives you **really really** do not want, but False Negative is more\n", "of a nuiscance. So you decide on the following error function (as we saw\n", "in the lecture) $$\\mathrm{e}_s(f(x),h(x)) =\n", "\\left\\{\n", "  \\begin{array}{l l}\n", "    0 & \\mathrm{ if  } \\quad f(x) = h(x)\\\\\n", "    1 & \\mathrm{ if  } \\quad f(x) =1, h(x) = 0\\\\\n", "    1000 & \\mathrm{ if  } \\quad f(x) =0, h(x) = 1\n", "  \\end{array}\n", "  \\right.$$ All you know is Logistic Regression (the standard one with\n", "0-1 labels from the Machine Learning class) which you train on the data\n", "given to you and learn the distribution $p(y \\mid x)$ (at least\n", "approximately). \n", "\n", "* How can we use our learned hypothesis to try and\n", "minimize the biased cost function $\\mathrm{e}_s$ instead of $0/1$ loss\n", "$(e(x,y) = 1_{[x\\neq y]})$ where we would return the most likely class?\n", "* Is there a general approach for other methods than logistic regression as long as we estimate probabilities?\n", "\n", "If it helps you write python code below that computes the cost of returning 0 and the cost of returning 1.\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["# python box\n", "### YOUR CODE HERE\n", "### END CODE\n", "# python box"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Ex 4: Generalization\n", "## Ben gets a great idea (generalization) \n", "Ben is still happily following the '5 ECTS ON ADVANCED BIRDS' course. The exam is written, and the professor  gave Ben several old exams sets he can practice on. One day Ben got a brilliant idea. Why couldn't the final exam be one of the tests he already practiced on? Very happy with himself, Ben asked his professor\n", "\n", "Ben: \"Professor. I got a brilliant idea. See this exam from last year? I solved it perfectly!\" \n", "\n", "Professor: \"Very good Ben. I am very impressed. \"\n", "\n", "Ben: \"Why don't you just re-use the exam from last year? This way I'll get a 12 and your boss will think you're the greatest teacher ever!\"\n", "\n", "Professor: \"I see your point Ben. Unfortunately there is one problem. If I did this, I would not be testing what you learned, but what you remembered!\"\n", "\n", "Ben: \"I am not sure I understand, professor. \"\n", "\n", "Professor: \"When I gave you the previous exams, you also got the solutions, right?\"\n", "\n", "Ben: \"Yes, this was very helpfull. \"\n", "\n", "Professor: \"If I give you the same exam, you could get a perfect score by just remembering the solutions. But I don't want to test what you remembered, I want to test if you learned something. \"\n", "\n", "Ben: \"I see. Continue. \"\n", "\n", "Professor: \"You can't solve a new exam by remembering. You never saw it before! My hope is that by doing the previous years exams you learned something and the final exam tests how well you learned. \"\n", "\n", "Ben: \"Interesting. So my performance on the previous exams and the final exam set are different things?\"\n", "\n", "Professor: \"Exactly! If you were taking a course in machine learning, we would call your performance on the previous exams *in-sample error* and denote it by $E_{in}$. \"\n", "\n", "Ben: \"So I can get perfect *in-sample error*, basically zero error, by just remembering all the correct solutions?\"\n", "\n", "Professor: \"Exactly! You would get $E_{in}=0$. But what I really care about is how well you perform on exam tests you never saw before. We call this *out-of-sample error* and denote it by $E_{out}$. \"\n", "\n", "Ben: \"I see. But I feel like my performance on previous exams give me a good idea on how I perform later. It seems like $E_{in}$ and $E_{out}$ are close to each other. \"\n", "\n", "Professor: \"This is often the case. We say that learning generalizes if $E_{in}$ and $E_{out}$ are close to each other, i.e. $|E_{in}-E_{out}|$ is small. \"\n", "\n", "Ben: \"But then how do we know this is the case. \"\n", "\n", "Professor: \"If you followed the machine learning course or read 'Learning from Data', you would know that the Hoeffding bound tells us that $\\Pr[\\mid E_{in}-E_{out}\\mid > \\epsilon] \\le 2M e^{-2\\epsilon^2 N}$. \"\n", "\n", "Ben: \"I see. This was very helpfull professor, but I'm not exactly sure what $M$ and $N$ are. \"\n", "\n", "Professor: \"I'd recommend you read pages 18-24 of 'Learning From Data' then. \"\n", "\n", "\n", "\n", "## Questions:\n", "\n", "<b>Question 1: </b> \n", "Does the Hoeffding bound give any meaningful bounds on $E_{in}$ and $E_{out}$ for the perceptron learning model?  \n", "\n", "HINT: Remember that the hypotesis set is $H=\\{ h(x)=\\text{sign}(w^\\intercal x)\\mid w\\in \\mathbb{R}^d\\}$ and $M=|H|$.<br><br>\n", "    \n", "<b>Question 2: </b>\n", "We want to learn $f:X \\rightarrow \\{0,\\;1\\}$. Assume $f$ is probabilistic and independent of the input domain $X$ such that\n", "\n", "$$P(f(x)=1\\mid x) = P(f(x) =1) = 0.2$$\n", "\n", "I.e. the probability that $f(x)=1$ is independent of $x$. You can think of this as follows: when $f$ is evaluated it ignores the input and flips its private biased coin independently and returns the result. \n", "\n", "<b>a)</b> What is the best out-of-sample error we can achieve? \n", "\n", "HINT: Consider $h(x)=1$ and $h(x)=0$. Would a randomized function be better? E.g. $h(x)=\\begin{cases}\n", "1\\quad\\text{with probability } 0.2\\\\\n", "0\\quad\\text{with probability } 0.8\n", "\\end{cases}$\n", "\n", "<b>b)</b> What if $P(f(x)=1) =0.5$? \n", "\n", "<b>c)</b> What are the optimal classifiers for these two cases?\n", "\n", "<b>d)</b> Read and understand the following code. Try experiment with different functions. Do the experiments match your answers to previous questions?"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "# The unknown target function. \n", "def f(x):\n", "    r = np.random.rand(1)\n", "    return r < 0.8\n", "\n", "# Our hypothesis (our guess)\n", "def h(x): \n", "    return 1\n", "\n", "# Lets try to estimate the error of h. \n", "data = np.random.rand(1000)\n", "f_data = np.array([f(x) for x in data])\n", "h_data = np.array([h(x) for x in data])\n", "print('Sample estimate of out of sample error ...')\n", "eout_sample = (f_data==h_data).mean()\n", "print('Wait for it some more....\\nIt is: {0}, Is that close? Can you make a better h?'.format(eout_sample))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Ex 5: In Sample Error\n", "Assume we are given a fixed hypothesis h, and we are considering 0-1 loss ($1_{h(x)\\neq y}$).\n", "Now we receive a sample data set $D = {(x_1,y_1),\\dots,(x_n,y_n)}$ where each data point is generated by sampling $x_i$  independently at random from unknown distribution $P(X)$ and then fed to the unknown $f$ to get $(x_i, y_i=f(x_i))$ .\n", "\n", "\n", "Show that the expected value (over the data set) of $E_{in}(h) = \\frac{1}{n} \\sum_{i=1}^n 1_{h(x_i)\\neq y_i}$ ? (number of mispredictions/number of points) is $E_{out}(h)$.\n", "\n", "Formally show that\n", "$$\n", "\\mathbb{E}_D [\\textrm{E}_\\textrm{in}(h)] = \\textrm{E}_\\textrm{out}(h)\n", "$$\n", "\n", "# Ex 6: Out of sample error \n", "Given that the target function is actually the the noisy linear model, $P(y\\mid x,w) = w^\\intercal x + \\varepsilon$ \n", "\n", "\n", "Given that we use the least squares error function, $e(x,y) = (x-y)^2$,\n", "what is the best **out of sample error** possible?\n", "\n", "Hint: What is the optimal classifier? What is the out of sample error of\n", "that one?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Lets plot some noisy data. It is easy....\n", "%matplotlib inline\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "sigma = np.pi\n", "n = 30\n", "data = np.c_[np.ones(n), np.linspace(-10, 10, n)]\n", "w = [np.e, np.pi]\n", "target = data @ w + sigma * np.random.randn(n)\n", "plt.plot(data[:,1], target, 'bx', label='Linear target function with noise')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ex 7: Break Points and Growth Functions \n", "\n", "-   Is there always a break point for a finite hypothesis set of $n$\n", "    hypotheses? If so, can you give a an upper bound? What is the growth\n", "    function?\n", "\n", "-   Does the set of all functions have a break point? What is its growth\n", "    function?\n", "\n", "-   What is the (smallest) break point for the hypothesis set consisting\n", "    of circles centered around $(0,0)$? For a given circle the\n", "    hypothesis returns $1$ for points inside the circle and $-1$ for\n", "    points outside. What is the growth function?\n", "\n", "-   What if we move to balls in the 3-dimensional space\n", "    ${{\\mathbb R}}^3$? Or in general $d$-dimensional space\n", "    ${{\\mathbb R}}^d$ (hyperspheres)?\n", "\n", "-  Show that the growth function for a singleton hypothesis clas $H = \\{h\\}$ is 0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ex 8: VC Dimension \n", "\n", "-   Does VC Dimension depend on the learning algorithm or the actual\n", "    data set given?\n", "\n", "-   Does VC Dimension depend on the probability distribution generating\n", "    the data (not the labels)?\n", "\n", "-   If $\\mathcal{H}_1 \\subseteq \\mathcal{H}_2$ is\n", "    $VC(\\mathcal{H}_1) \\leq VC(\\mathcal{H}_2)?$\n", "\n", "-   Can you give an upper bound on the VC dimension of a finite set of\n", "    $M$ hypotheses?\n", "\n", "-   What is the VC Dimension for the hypothesis set consisting of\n", "    circles centered around 0?\n", "\n", "-   What if we move to balls (3d)? or in general d dimensions\n", "    (hypershperes)?\n", "\n", "-   What is the maximal VC dimension possible of the intersection of\n", "    hypothesis spaces $\\mathcal{H}_1,\\dots,\\mathcal{H}_n$ with VC\n", "    dimension $v_1,\\dots,v_n$.\n", "\n", "-   As previous question, instead what is the minimal VC dimension of\n", "    the union of the hypothesis spaces from the previous question\n", "\n", "\n", "    \n", "-   Show that the VC dimension the hypothesis set consisting of axis aligned rectangles in $\\mathbb{R}^2$ is 4,\n", "    i.e. find a point set of 4 points you can shatter and argue that any point set of size 5 can not.\n", "\n", "-   At the lecture  we showed that we could learn a Rectangle by computing the minimum boundound box with the following generalization bound.\n", "    $$ P(E_\\textrm{out} > \\varepsilon) \\leq 4 e^{-\\varepsilon n/4} $$\n", "    Rewrite this bound and the VC dimension bound we get from the previous exercise to generalization bounds and compare.\n", "    The generalization bound (LFD 2.2.2) says that with probability $1-\\delta$\n", "    $$\n", "    E_\\textrm{out} \\leq E_{in} + \\textrm{ something}\n", "    $$\n", "    Compare the two bounds. Which is better and by how much. What differences are there?\n", "    \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ex 9: VC Dimension of Hyperplanes (Book Exercise 2.4 p. 52)\n", "Show that the VC dimension of the hypothesis space $\\mathcal{H} = \\{\\textrm{sign}(w^\\intercal x) \\mid w\\in \\mathbb{R}^d \\}$ is $d+1$.\n", "\n", "We need to show \n", "1. That there exists a data set of size d+1 that can be shattered by hyperplanes\n", "2. That no data set of size d+2 can be shattered by hyperplanes\n", "\n", "We will give a few more hints than the book does.\n", "### Shattering d+1 points\n", "As the book hints you must create an \"easy\" data set that you store in matrix X. Here easy means X should be invertible.\n", "\n", "**Hint:** We suggest you consider as a data matrix, the identity matrix with the first column set to ones (which it needs to be). This essentially means that each data point has its own dimension.\n", "Now you must show how to get any dichotomy, call that $y \\in \\{-1,1\\}^{d+1}$.\n", "To be precise you must find $w$ such that \n", "$$\n", "    y_i = \\textrm{sign}(w^\\intercal x_i) \n", "$$\n", "for $i=1,\\dots,d+1$.\n", "In matrix notation this is \n", "$$\n", "Y = \\textrm{Sign}(Xw)\n", "$$\n", "\n", "The final hint you get here. Make this nonlinear system of equations into a linear one and show how to get a $w$ that works.\n", "### No Shattering of d+2 points.\n", "Must show that for any d+2 points we must prove there is a  dichotomy hyperplanes can not capture.\n", "\n", "**Hint:**\n", "\n", "Consider d+2, points $x_1,\\dots, x_{d+2}$ of dimension (d+1) and think of them as vectors in $\\mathbb{R}^{d+1}$.\n", "Since we have more vectors than dimensions the vectors must be linearly dependent.\n", "\n", "i.e. \n", "$$\n", "x_j = \\sum_{i\\neq j} a_i x_i\n", "$$\n", "Since $x_j$ is determined by the other data points then so is $w^\\intercal x_j$ for any $w$. This means the classification on point $x_j$ is dictated by the classification of the data other points and thus cannot freely be chosen.\n", "I.e.\n", "$$\n", "w^\\intercal x_j = w^\\intercal \\sum_{i\\neq j} a_i x_i =\\sum_{i\\neq j} a_i w^\\intercal x_i\n", "$$\n", "Define an impossible dichotomy as follows. \n", "$$\n", "y_i = \\textrm{sign}(a_i), \\quad i\\neq j, \\quad y_j = -1\n", "$$\n", "Show this dichotomy is impossible!\n"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["## Ex: 10 Bayes Error - The Optimal Classifier\n", "We consider 0-1 Loss i.e. $1_{f(x)\\neq h(x)}$\n", "\n", "Given a distribution $D$ over $X \\times Y$, the Bayes error $E^\u2217$ is defined as the infimum of the errors achieved by measurable (technical, ignore think \"reasonable\") functions $h: X \\rightarrow Y$:\n", "$$\n", "E^* = \\inf_h E_\\textrm{out}(h).\n", "$$\n", "A hypothesis $h$ with $E_{\\textrm{out}}(h) = R^*$ is called a Bayes hypothesis or Bayes classifier, which is the optimal classifier which achieves the optimal error.\n", "\n", "If the target function $f$ is deterministic (i.e. given input x it always outputs some fixed value y=f(x)) then what is the Bayes Error?\n", "\n", "Now let us consider a probabilistic target function. Assume the target probability distribution  is the following \"noisy function\" defined from a deterministic function $f: X\\rightarrow \\{0,1\\}$.\n", "\n", "$P(y \\mid x)$ is defined as follows.\n", "Given $x$ the compute value $f(x)$. With probability $\\theta=0.9$ return $f(x)$ and with probability $1-\\theta=0.1$ return $1-f(x)$\n", "\n", "What is the Bayes Error of this function? What is a Bayes classifier?\n", "What if we set $\\theta=0.5$ or $\\theta = 0.9$?\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["## Ex 11: Book Problem 2.18 In short\n", "$$\n", "\\mathcal{H}= \\{h_\\alpha \\mid h_\\alpha(x) = (-1)^{\\lfloor \\alpha\n", "          x\\rfloor}, \\alpha \\in {{\\mathbb R}}\\}\n", "$$ \n", "\n", "Show that the VC dimension of ${{\\mathcal H}}$ is infinite (even tough there is only one parameter!)\n", "\n", "    Hint: Use the points set\n", "$x_1=10,x_2=100,\\dots,x_i = 10^i,\\dots,x_N=10^N$ and show how to implement any dichotomi $y_1,\\dots,y_N$ (find $\\alpha$ that works).\n", "You can safely assume $\\alpha >0$."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.0"}}, "nbformat": 4, "nbformat_minor": 2}